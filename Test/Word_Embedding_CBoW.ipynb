{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "528c12d8",
   "metadata": {},
   "source": [
    "# Word Embedding and CBoW (Continuous Bag of Words)\n",
    "\n",
    "This notebook demonstrates the step-by-step process of building a vocabulary, creating context-target pairs, and explaining the CBoW architecture using a sample sentence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4898a8fd",
   "metadata": {},
   "source": [
    "## Step 1: Select a Text\n",
    "\n",
    "Sample: \"AI and Data Science have become important and popular topics in Cambodia today.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d921af93",
   "metadata": {},
   "source": [
    "## Step 2: Tokenize the Text\n",
    "\n",
    "We break the text into individual words (tokens) using NLTK.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "350bce43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['AI', 'and', 'Data', 'Science', 'have', 'become', 'important', 'and', 'popular', 'topics', 'in', 'Cambodia', 'today', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "\n",
    "sample_text = \"AI and Data Science have become important and popular topics in Cambodia today.\"\n",
    "tokens = nltk.word_tokenize(sample_text)\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e38b473",
   "metadata": {},
   "source": [
    "## Step 3: Build the Vocabulary\n",
    "\n",
    "We list all unique words and assign each word an index number.\n",
    "\n",
    "**How to build the vocabulary?**\n",
    "\n",
    "- Extract all unique words from the token list.\n",
    "- Assign a unique index to each word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a768e3a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: {'.': 0, 'AI': 1, 'Cambodia': 2, 'Data': 3, 'Science': 4, 'and': 5, 'become': 6, 'have': 7, 'important': 8, 'in': 9, 'popular': 10, 'today': 11, 'topics': 12}\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary\n",
    "token_set = sorted(set(tokens))\n",
    "vocab = {word: idx for idx, word in enumerate(token_set)}\n",
    "print(\"Vocabulary:\", vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc66a64a",
   "metadata": {},
   "source": [
    "## Step 4: Set the Context Window\n",
    "\n",
    "We choose how many words before and after the target word to use as context. Here, we use a window size of 2.\n",
    "\n",
    "**Why set context window?**\n",
    "\n",
    "- The context window determines how much surrounding information is used to predict the target word. A larger window captures broader context, while a smaller window focuses on local context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25de0cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context window size: 2\n"
     ]
    }
   ],
   "source": [
    "window_size = 2\n",
    "print(f\"Context window size: {window_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0767064e",
   "metadata": {},
   "source": [
    "## Step 5: Identify Target & Context Words\n",
    "\n",
    "For each word, we mark it as the target and write down the surrounding context words.\n",
    "\n",
    "**Why take the surrounding words?**\n",
    "\n",
    "- The surrounding words provide context, helping the model learn the meaning and relationships between words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4932825b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: AI, Context: ['and', 'Data']\n",
      "Target: and, Context: ['AI', 'Data', 'Science']\n",
      "Target: Data, Context: ['AI', 'and', 'Science', 'have']\n",
      "Target: Science, Context: ['and', 'Data', 'have', 'become']\n",
      "Target: have, Context: ['Data', 'Science', 'become', 'important']\n",
      "Target: become, Context: ['Science', 'have', 'important', 'and']\n",
      "Target: important, Context: ['have', 'become', 'and', 'popular']\n",
      "Target: and, Context: ['become', 'important', 'popular', 'topics']\n",
      "Target: popular, Context: ['important', 'and', 'topics', 'in']\n",
      "Target: topics, Context: ['and', 'popular', 'in', 'Cambodia']\n",
      "Target: in, Context: ['popular', 'topics', 'Cambodia', 'today']\n",
      "Target: Cambodia, Context: ['topics', 'in', 'today', '.']\n",
      "Target: today, Context: ['in', 'Cambodia', '.']\n",
      "Target: ., Context: ['Cambodia', 'today']\n"
     ]
    }
   ],
   "source": [
    "# Identify target and context words\n",
    "target_context_pairs = []\n",
    "for idx, word in enumerate(tokens):\n",
    "    start = max(0, idx - window_size)\n",
    "    end = min(len(tokens), idx + window_size + 1)\n",
    "    context = [tokens[i] for i in range(start, end) if i != idx]\n",
    "    target_context_pairs.append((word, context))\n",
    "\n",
    "for pair in target_context_pairs:\n",
    "    print(f\"Target: {pair[0]}, Context: {pair[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d078d6ad",
   "metadata": {},
   "source": [
    "## Step 6: Create Input-Output Pairs\n",
    "\n",
    "For CBoW, the context words are the input and the target word is the output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cdc6eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input (Context): ['and', 'Data'], Output (Target): AI\n",
      "Input (Context): ['AI', 'Data', 'Science'], Output (Target): and\n",
      "Input (Context): ['AI', 'and', 'Science', 'have'], Output (Target): Data\n",
      "Input (Context): ['and', 'Data', 'have', 'become'], Output (Target): Science\n",
      "Input (Context): ['Data', 'Science', 'become', 'important'], Output (Target): have\n",
      "Input (Context): ['Science', 'have', 'important', 'and'], Output (Target): become\n",
      "Input (Context): ['have', 'become', 'and', 'popular'], Output (Target): important\n",
      "Input (Context): ['become', 'important', 'popular', 'topics'], Output (Target): and\n",
      "Input (Context): ['important', 'and', 'topics', 'in'], Output (Target): popular\n",
      "Input (Context): ['and', 'popular', 'in', 'Cambodia'], Output (Target): topics\n",
      "Input (Context): ['popular', 'topics', 'Cambodia', 'today'], Output (Target): in\n",
      "Input (Context): ['topics', 'in', 'today', '.'], Output (Target): Cambodia\n",
      "Input (Context): ['in', 'Cambodia', '.'], Output (Target): today\n",
      "Input (Context): ['Cambodia', 'today'], Output (Target): .\n"
     ]
    }
   ],
   "source": [
    "# Create input-output pairs for CBoW\n",
    "cbow_pairs = []\n",
    "for target, context in target_context_pairs:\n",
    "    cbow_pairs.append((context, target))\n",
    "\n",
    "for pair in cbow_pairs:\n",
    "    print(f\"Input (Context): {pair[0]}, Output (Target): {pair[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291e7576",
   "metadata": {},
   "source": [
    "## Step 7: Draw the CBoW Architecture\n",
    "\n",
    "Below is a simple flowchart of the CBoW model:\n",
    "\n",
    "```\n",
    "Input Layer (Context Words)\n",
    "        |\n",
    "        v\n",
    "   Hidden Layer (Embeddings)\n",
    "        |\n",
    "        v\n",
    "Output Layer (Softmax predicts Target Word)\n",
    "```\n",
    "\n",
    "- **Input Layer:** Takes context words as input.\n",
    "- **Hidden Layer:** Learns word embeddings.\n",
    "- **Output Layer:** Uses softmax to predict the target word.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5fba78",
   "metadata": {},
   "source": [
    "## Step 8: Explain the Learning Process\n",
    "\n",
    "The CBoW model learns to predict the target word from its context using a softmax function at the output layer. During training, **backpropagation** is used to update the weights in the network, minimizing the prediction error and improving the quality of the word embeddings.\n",
    "\n",
    "- The model takes context words as input.\n",
    "- It computes the average of their embeddings in the hidden layer.\n",
    "- The output layer predicts the target word using softmax.\n",
    "- Backpropagation adjusts the weights to reduce the difference between the predicted and actual target word.\n",
    "\n",
    "This process is repeated for all input-output pairs, resulting in meaningful word embeddings.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
