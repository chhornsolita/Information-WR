{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dde5b6c",
   "metadata": {},
   "source": [
    "# Text and Document Preparation\n",
    "\n",
    "This notebook demonstrates a complete workflow for text and document preparation using a sample document. Each section covers a key step in the process, with code, explanations, and answers to logic questions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d012e80f",
   "metadata": {},
   "source": [
    "## 1. Tokenization\n",
    "\n",
    "We begin by tokenizing the sample document into sentences and words. Tokenization is the process of splitting text into smaller units (tokens), such as sentences or words.\n",
    "\n",
    "**Sample Document:**\n",
    "\n",
    "> Artificial intelligence (AI) is revolutionizing various industries by providing innovative solutions. AI and & Data Science are the technologies, and very hot topic in Cambodia.\n",
    "\n",
    "**Questions:**\n",
    "\n",
    "- What types of tokenization will you apply? (sentence, word, or both)\n",
    "- Why is tokenization important in IR?\n",
    "- What is `word_tokenize()` and `sent_tokenize()`?\n",
    "\n",
    "**Answers:**\n",
    "\n",
    "- Both sentence and word tokenization will be applied. Sentence tokenization splits text into sentences, while word tokenization splits sentences into words.\n",
    "- Tokenization is important in Information Retrieval (IR) because it enables systems to process and analyze text at the word or sentence level, improving search and retrieval accuracy.\n",
    "- `word_tokenize()` splits text into words; `sent_tokenize()` splits text into sentences.\n",
    "\n",
    "Let's see this in code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fe88db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Tokenization: ['Artificial intelligence (AI) is revolutionizing various industries by providing innovative solutions.', 'AI and & Data Science are the technologies, and very hot topic in Cambodia.']\n",
      "Word Tokenization (nltk): ['Artificial', 'intelligence', '(', 'AI', ')', 'is', 'revolutionizing', 'various', 'industries', 'by', 'providing', 'innovative', 'solutions', '.', 'AI', 'and', '&', 'Data', 'Science', 'are', 'the', 'technologies', ',', 'and', 'very', 'hot', 'topic', 'in', 'Cambodia', '.']\n",
      "Word Tokenization (scikit-learn): ['ai' 'and' 'are' 'artificial' 'by' 'cambodia' 'data' 'hot' 'in'\n",
      " 'industries' 'innovative' 'intelligence' 'is' 'providing'\n",
      " 'revolutionizing' 'science' 'solutions' 'technologies' 'the' 'topic'\n",
      " 'various' 'very']\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#nltk.download('punkt')\n",
    "sample_doc = \"Artificial intelligence (AI) is revolutionizing various industries by providing innovative solutions. AI and & Data Science are the technologies, and very hot topic in Cambodia.\"\n",
    "\n",
    "# Sentence tokenization\n",
    "sentences = nltk.sent_tokenize(sample_doc)\n",
    "print(\"Sentence Tokenization:\", sentences)\n",
    "\n",
    "# Word tokenization (nltk)\n",
    "words_nltk = nltk.word_tokenize(sample_doc)\n",
    "print(\"Word Tokenization (nltk):\", words_nltk)\n",
    "\n",
    "# Word tokenization (scikit-learn)\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform([sample_doc])\n",
    "words_sklearn = vectorizer.get_feature_names_out()\n",
    "print(\"Word Tokenization (scikit-learn):\", words_sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0506325",
   "metadata": {},
   "source": [
    "## 2. Stop-word Removal\n",
    "\n",
    "Stop-words are common words (such as 'the', 'is', 'and') that are often removed from text to focus on meaningful content. Domain-specific stopwords (e.g., 'AI', 'Data') may also be considered.\n",
    "\n",
    "**Questions:**\n",
    "\n",
    "- List the stop-words you removed, including domain-specific stopwords.\n",
    "- Explain how removing stop-words impacts understanding and processing of text.\n",
    "- Why remove contextual stopwords in IR systems?\n",
    "\n",
    "**Answers:**\n",
    "\n",
    "- Removing stop-words reduces noise and improves the relevance of retrieved information. Contextual stopwords (domain-specific) help tailor retrieval to the subject area.\n",
    "\n",
    "Let's remove stop-words from our tokenized words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43eb7c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop-words removed: ['a', 'about', 'above', 'after', 'again', 'against', 'ai', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'data', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", \"he's\", 'her', 'here', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'if', 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'science', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', \"should've\", 'shouldn', \"shouldn't\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'were', 'weren', \"weren't\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves']\n",
      "Filtered words: ['artificial', 'cambodia', 'hot', 'industries', 'innovative', 'intelligence', 'providing', 'revolutionizing', 'solutions', 'technologies', 'topic', 'various']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "# Standard English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# Add domain-specific stopwords\n",
    "domain_stopwords = {'ai', 'data', 'science'}\n",
    "stop_words.update(domain_stopwords)\n",
    "\n",
    "# Remove stopwords from scikit-learn tokenized words\n",
    "filtered_words = [word for word in words_sklearn if word.lower() not in stop_words]\n",
    "print(\"Stop-words removed:\", sorted(domain_stopwords.union(stopwords.words('english'))))\n",
    "print(\"Filtered words:\", filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9960376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom stop-words: {'ai', 'the', 'by', 'is', 'data', 'in', 'and', 'are'}\n",
      "Filtered words: ['artificial', 'cambodia', 'hot', 'industries', 'innovative', 'intelligence', 'providing', 'revolutionizing', 'science', 'solutions', 'technologies', 'topic', 'various', 'very']\n"
     ]
    }
   ],
   "source": [
    "# Custom stop-word removal\n",
    "custom_stopwords = {'is', 'the', 'are', 'and', 'in', 'by', 'ai', 'data'}\n",
    "\n",
    "# Remove custom stopwords from scikit-learn tokenized words\n",
    "filtered_words = [word for word in words_sklearn if word.lower() not in custom_stopwords]\n",
    "print(\"Custom stop-words:\", custom_stopwords)\n",
    "print(\"Filtered words:\", filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d786d8a4",
   "metadata": {},
   "source": [
    "## 3. Stemming and Lemmatization\n",
    "\n",
    "Stemming reduces words to their root form, while lemmatization converts words to their base or dictionary form.\n",
    "\n",
    "**Questions:**\n",
    "\n",
    "- Select one word and show its stemmed and lemmatized forms.\n",
    "- What are the advantages and disadvantages of stemming vs. lemmatization?\n",
    "- When is stemming preferred over lemmatization?\n",
    "\n",
    "**Answers:**\n",
    "\n",
    "- Stemming is faster but less accurate; lemmatization is more accurate but slower. Stemming may be preferred for speed in large-scale IR systems.\n",
    "\n",
    "Let's apply both to our filtered words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36944fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original word: artificial\n",
      "Stemmed form: artifici\n",
      "Lemmatized form: artificial\n",
      "Stemmed words: ['artifici', 'cambodia', 'hot', 'industri', 'innov', 'intellig', 'provid', 'revolution', 'solut', 'technolog', 'topic', 'variou']\n",
      "Lemmatized words: ['artificial', 'cambodia', 'hot', 'industry', 'innovative', 'intelligence', 'providing', 'revolutionizing', 'solution', 'technology', 'topic', 'various']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Select one word for demonstration\n",
    "demo_word = filtered_words[0] if filtered_words else 'revolutionizing'\n",
    "stemmed = stemmer.stem(demo_word)\n",
    "lemmatized = lemmatizer.lemmatize(demo_word)\n",
    "\n",
    "print(f\"Original word: {demo_word}\")\n",
    "print(f\"Stemmed form: {stemmed}\")\n",
    "print(f\"Lemmatized form: {lemmatized}\")\n",
    "\n",
    "# Apply stemming and lemmatization to all filtered words\n",
    "stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "print(\"Stemmed words:\", stemmed_words)\n",
    "print(\"Lemmatized words:\", lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e2430e",
   "metadata": {},
   "source": [
    "## 4. Normalization and Case Folding\n",
    "\n",
    "Normalization and case folding convert all words to lowercase and standardize text format.\n",
    "\n",
    "**Question:**\n",
    "\n",
    "- How does normalization help reduce vocabulary size?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "- Normalization reduces vocabulary size by treating words like \"AI\" and \"ai\" as the same token, improving consistency and reducing redundancy.\n",
    "\n",
    "Let's normalize our lemmatized words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8cc78288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized words: ['artificial', 'cambodia', 'hot', 'industry', 'innovative', 'intelligence', 'providing', 'revolutionizing', 'solution', 'technology', 'topic', 'various']\n"
     ]
    }
   ],
   "source": [
    "# Normalize and case fold\n",
    "normalized_words = [word.lower() for word in lemmatized_words]\n",
    "print(\"Normalized words:\", normalized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508c33de",
   "metadata": {},
   "source": [
    "## 5. Handling Special Characters, Numbers, and Punctuation\n",
    "\n",
    "Cleaning text involves removing special characters, numbers, and punctuation to focus on meaningful words.\n",
    "\n",
    "**Questions:**\n",
    "\n",
    "- Which characters will you remove? Why?\n",
    "- What problems may occur if special characters are not removed?\n",
    "\n",
    "**Answers:**\n",
    "\n",
    "- Remove characters like `&`, `(`, `)`, `.`, `,` and numbers because they do not contribute to semantic meaning and may introduce noise.\n",
    "- If not removed, special characters can cause errors in analysis, inflate vocabulary size, and reduce retrieval accuracy.\n",
    "\n",
    "Let's clean the normalized words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e885188e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned words: ['artificial', 'cambodia', 'hot', 'industry', 'innovative', 'intelligence', 'providing', 'revolutionizing', 'solution', 'technology', 'topic', 'various']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# Remove special characters, numbers, and punctuation from normalized words\n",
    "cleaned_words = [re.sub(r'[^a-z]', '', word) for word in normalized_words if re.sub(r'[^a-z]', '', word)]\n",
    "print(\"Cleaned words:\", cleaned_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad73ae85",
   "metadata": {},
   "source": [
    "## 6. Flowchart: Text Processing Workflow\n",
    "\n",
    "Below is a flowchart illustrating the text processing steps described above.\n",
    "\n",
    "```\n",
    "Sample Document\n",
    "     |\n",
    "     v\n",
    "Tokenization (Sentence & Word)\n",
    "     |\n",
    "     v\n",
    "Stop-word Removal\n",
    "     |\n",
    "     v\n",
    "Stemming & Lemmatization\n",
    "     |\n",
    "     v\n",
    "Normalization & Case Folding\n",
    "     |\n",
    "     v\n",
    "Remove Special Characters, Numbers, Punctuation\n",
    "     |\n",
    "     v\n",
    "Cleaned Tokens\n",
    "```\n",
    "\n",
    "This workflow ensures that text is prepared for further analysis or information retrieval tasks.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
