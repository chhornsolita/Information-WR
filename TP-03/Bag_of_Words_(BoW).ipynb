{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "701fd498",
   "metadata": {},
   "source": [
    "# 1: Implement Bag of Words (BoW) with Python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c421326e",
   "metadata": {},
   "source": [
    "- Note use custom stopword instead of nltk(NLTK stopwords list contains 179 words, including:'a', 'an', 'the', 'is', 'to', 'now', 'this', 'it', 'for', ...\n",
    "  ) because we want the output to match the expected output exactly in the task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25e21014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['welcome', 'to', 'nlp', 'learning', ',', 'now', 'start', 'learning']\n",
      "['learning', 'is', 'a', 'good', 'practice']\n",
      "['welcome', 'to', 'nlp', 'learning', ',', 'now', 'start', 'is', 'a', 'good', 'practice']\n",
      "['welcome', 'nlp', 'learning', 'now', 'start', 'good', 'practice']\n",
      "[1, 1, 2, 1, 1, 0, 0]\n",
      "[0, 0, 1, 0, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "sentence1 = \"Welcome to NLP Learning , Now start learning\"\n",
    "sentence2 = \"Learning is a good practice\"\n",
    "\n",
    "# 1. Tokenize sentences (keep comma)\n",
    "def tokenize(text):\n",
    "    # keep words + comma\n",
    "    return re.findall(r\"[A-Za-z]+|,\", text.lower())\n",
    "\n",
    "tokens1 = tokenize(sentence1)\n",
    "tokens2 = tokenize(sentence2)\n",
    "\n",
    "print(tokens1)\n",
    "print(tokens2)\n",
    "\n",
    "# 2. Combined vocabulary (preserve ORDER)\n",
    "vocab = []\n",
    "for word in tokens1 + tokens2:\n",
    "    if word not in vocab:\n",
    "        vocab.append(word)\n",
    "\n",
    "print(vocab)\n",
    "\n",
    "# 3. Final vocabulary (remove punctuation + small stopwords 'to','is','a')\n",
    "final_vocab = [w for w in vocab if w.isalpha() and w not in ['to','is','a']]\n",
    "print(final_vocab)\n",
    "\n",
    "# 4. Create Bag of Words vector\n",
    "def bow_vector(tokens, vocab):\n",
    "    return [tokens.count(word) for word in vocab]\n",
    "\n",
    "v1 = bow_vector(tokens1, final_vocab)\n",
    "v2 = bow_vector(tokens2, final_vocab)\n",
    "\n",
    "print(v1)\n",
    "print(v2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c31de0e",
   "metadata": {},
   "source": [
    "# 2. Implement Bag of Words using SKLEARN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2efe1077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   good  job  miss\n",
      "0     1    1     1\n",
      "1     1    0     0\n"
     ]
    }
   ],
   "source": [
    "# Task 2: Bag of Words Implementation using SKLEARN\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample sentences\n",
    "sentence1 = \"This is a good job. I will not miss it for anything\"\n",
    "sentence2 = \"This is not good at all\"\n",
    "\n",
    "# Create a CountVectorizer instance\n",
    "vectorizer = CountVectorizer(vocabulary= [\"good\" , \"job\", \"miss\"])\n",
    "\n",
    "# Fit and transform the sentences\n",
    "corpus = [sentence1, sentence2]\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Extract vocabulary and BoW vectors\n",
    "vocabulary = vectorizer.get_feature_names_out()\n",
    "\n",
    "result_df = pd.DataFrame(X.toarray(), \n",
    "                         columns= vectorizer.get_feature_names_out(), \n",
    "                         index=[0, 1])\n",
    "\n",
    "print(result_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3016d9",
   "metadata": {},
   "source": [
    "# 3. Implement Bag of words using NLTK\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a836d9",
   "metadata": {},
   "source": [
    "- this part use with nltk to automatically remove all stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12aa2fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words (BoW) representation:\n",
      "{'classification': 1, 'important': 1, 'language': 1, 'love': 1, 'natural': 1, 'nlp': 2, 'nltk': 1, 'processing': 1, 'provides': 1, 'task': 1, 'text': 1, 'tools': 1, 'useful': 1}\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import re\n",
    "\n",
    "# 1. Define a list of sample documents\n",
    "documents = [\n",
    "    \"I love natural language processing.\",\n",
    "    \"Text classification is an important NLP task.\",\n",
    "    \"NLTK provides useful tools for NLP.\"\n",
    "]\n",
    "\n",
    "# 2. Initialize a list to hold all processed tokens from all documents\n",
    "all_tokens = []\n",
    "\n",
    "# Get the list of English stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# 3. Process each document\n",
    "for doc in documents:\n",
    "    # Tokenize the document into words\n",
    "    tokens = word_tokenize(doc)\n",
    "\n",
    "    # Convert words to lowercase\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "\n",
    "    # Remove punctuation and stopwords from the tokens\n",
    "    # Also, remove any non-alphabetic characters\n",
    "    filtered_tokens = [\n",
    "        word for word in tokens\n",
    "        if word.isalpha() and word not in stop_words\n",
    "    ]\n",
    "\n",
    "    # Add the filtered tokens to our master list\n",
    "    all_tokens.extend(filtered_tokens)\n",
    "\n",
    "# 4. Create a vocabulary by collecting all unique words, sorted alphabetically\n",
    "vocabulary = sorted(list(set(all_tokens)))\n",
    "\n",
    "# 5. Initialize a BoW dictionary with word counts set to 0\n",
    "bow_representation = {word: 0 for word in vocabulary}\n",
    "\n",
    "# 6. Iterate through the collected tokens and increment the count for each word\n",
    "for token in all_tokens:\n",
    "    bow_representation[token] += 1\n",
    "\n",
    "# 7. Print the final Bag of Words representation\n",
    "print(\"Bag of Words (BoW) representation:\")\n",
    "print(bow_representation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0da9738",
   "metadata": {},
   "source": [
    "get this result becuase use funtion sort to sort alphabet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87151deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words (BoW) representation:\n",
      "{'processing': 1, 'provides': 1, 'text': 1, 'natural': 1, 'nlp': 2, 'language': 1, 'useful': 1, 'nltk': 1, 'tools': 1, 'task': 1, 'important': 1, 'love': 1, 'classification': 1}\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import re\n",
    "\n",
    "# 1. Define a list of sample documents\n",
    "documents = [\n",
    "    \"I love natural language processing.\",\n",
    "    \"Text classification is an important NLP task.\",\n",
    "    \"NLTK provides useful tools for NLP.\"\n",
    "]\n",
    "\n",
    "# 2. Initialize a list to hold all processed tokens from all documents\n",
    "all_tokens = []\n",
    "\n",
    "# Get the list of English stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# 3. Process each document\n",
    "for doc in documents:\n",
    "    # Tokenize the document into words\n",
    "    tokens = word_tokenize(doc)\n",
    "\n",
    "    # Convert words to lowercase\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "\n",
    "    # Remove punctuation and stopwords from the tokens\n",
    "    # Also, remove any non-alphabetic characters\n",
    "    filtered_tokens = [\n",
    "        word for word in tokens\n",
    "        if word.isalpha() and word not in stop_words\n",
    "    ]\n",
    "\n",
    "    # Add the filtered tokens to our master list\n",
    "    all_tokens.extend(filtered_tokens)\n",
    "\n",
    "# 4. Create a vocabulary by collecting all unique words, do not sorted alphabetically\n",
    "vocabulary = list(set(all_tokens))\n",
    "\n",
    "# 5. Initialize a BoW dictionary with word counts set to 0\n",
    "bow_representation = {word: 0 for word in vocabulary}\n",
    "\n",
    "# 6. Iterate through the collected tokens and increment the count for each word\n",
    "for token in all_tokens:\n",
    "    bow_representation[token] += 1\n",
    "\n",
    "# 7. Print the final Bag of Words representation\n",
    "print(\"Bag of Words (BoW) representation:\")\n",
    "print(bow_representation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83c6b48",
   "metadata": {},
   "source": [
    "get the below output because do not use funtion sort to sort alphabet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e860c323",
   "metadata": {},
   "source": [
    "# 4 Classify movie review is posi ve or nega ve using Bag of words for pre-processing the\n",
    "\n",
    "text (from Sklearn) and apply with any models (RF, DT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70b41200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8467\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.86      0.85      4961\n",
      "           1       0.85      0.84      0.85      5039\n",
      "\n",
      "    accuracy                           0.85     10000\n",
      "   macro avg       0.85      0.85      0.85     10000\n",
      "weighted avg       0.85      0.85      0.85     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Task 4: Classify Movie Reviews using Bag of Words and Sklearn\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "# Note: Replace the path with the actual path to the downloaded dataset\n",
    "file_path = \"IMDB Dataset.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Step 2: Preprocess the text\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Tokenize, remove stopwords and punctuation, and convert to lowercase.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove punctuation\n",
    "    return text\n",
    "\n",
    "data['review'] = data['review'].apply(preprocess_text)\n",
    "\n",
    "# Step 3: Convert text to Bag of Words\n",
    "vectorizer = CountVectorizer(stop_words='english', max_features=5000)  # Limit to top 5000 words\n",
    "X = vectorizer.fit_transform(data['review'])\n",
    "y = data['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)  # Convert sentiment to binary\n",
    "\n",
    "# Step 4: Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 5: Train the model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 6: Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db1d653e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [0]\n"
     ]
    }
   ],
   "source": [
    "sample_review_1 = [\"This movie is terrible. I hated everything about it\"]\n",
    "sample_vec_1 = vectorizer.transform(sample_review_1)\n",
    "\n",
    "print(\"Prediction:\", model.predict(sample_vec_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c146d218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [1]\n"
     ]
    }
   ],
   "source": [
    "sample_review_2 = [\"I absolutely loved this movie! The acting was amazing\"]\n",
    "sample_vec_2 = vectorizer.transform(sample_review_2)\n",
    "\n",
    "print(\"Prediction:\", model.predict(sample_vec_2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
