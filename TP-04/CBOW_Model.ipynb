{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bfc76be",
   "metadata": {},
   "source": [
    "## Step 1: Preprocess Text\n",
    "\n",
    "We will preprocess the text by converting it to lowercase, removing punctuation, and splitting it into sentences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "61bd9e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "# Example text\n",
    "#text = \"Do all the good you can, for all the people you can, in all the ways you can, as long as you can.\"\n",
    "path = \"D:\\\\Y5 AMS\\\\Information-WR\\\\TP-04\\\\IMDB Dataset.csv\"\n",
    "text = pd.read_csv(path)[\"review\"][0]\n",
    "print(\"Data:\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e6b9f613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences: [['one', 'of', 'the', 'other', 'reviewers', 'has', 'mentioned', 'that', 'after', 'watching', 'just', '1', 'oz', 'episode', 'youll', 'be', 'hooked', 'they', 'are', 'right', 'as', 'this', 'is', 'exactly', 'what', 'happened', 'with', 'mebr', 'br', 'the', 'first', 'thing', 'that', 'struck', 'me', 'about', 'oz', 'was', 'its', 'brutality', 'and', 'unflinching', 'scenes', 'of', 'violence', 'which', 'set', 'in', 'right', 'from', 'the', 'word', 'go', 'trust', 'me', 'this', 'is', 'not', 'a', 'show', 'for', 'the', 'faint', 'hearted', 'or', 'timid', 'this', 'show', 'pulls', 'no', 'punches', 'with', 'regards', 'to', 'drugs', 'sex', 'or', 'violence', 'its', 'is', 'hardcore', 'in', 'the', 'classic', 'use', 'of', 'the', 'wordbr', 'br', 'it', 'is', 'called', 'oz', 'as', 'that', 'is', 'the', 'nickname', 'given', 'to', 'the', 'oswald', 'maximum', 'security', 'state', 'penitentary', 'it', 'focuses', 'mainly', 'on', 'emerald', 'city', 'an', 'experimental', 'section', 'of', 'the', 'prison', 'where', 'all', 'the', 'cells', 'have', 'glass', 'fronts', 'and', 'face', 'inwards', 'so', 'privacy', 'is', 'not', 'high', 'on', 'the', 'agenda', 'em', 'city', 'is', 'home', 'to', 'manyaryans', 'muslims', 'gangstas', 'latinos', 'christians', 'italians', 'irish', 'and', 'moreso', 'scuffles', 'death', 'stares', 'dodgy', 'dealings', 'and', 'shady', 'agreements', 'are', 'never', 'far', 'awaybr', 'br', 'i', 'would', 'say', 'the', 'main', 'appeal', 'of', 'the', 'show', 'is', 'due', 'to', 'the', 'fact', 'that', 'it', 'goes', 'where', 'other', 'shows', 'wouldnt', 'dare', 'forget', 'pretty', 'pictures', 'painted', 'for', 'mainstream', 'audiences', 'forget', 'charm', 'forget', 'romanceoz', 'doesnt', 'mess', 'around', 'the', 'first', 'episode', 'i', 'ever', 'saw', 'struck', 'me', 'as', 'so', 'nasty', 'it', 'was', 'surreal', 'i', 'couldnt', 'say', 'i', 'was', 'ready', 'for', 'it', 'but', 'as', 'i', 'watched', 'more', 'i', 'developed', 'a', 'taste', 'for', 'oz', 'and', 'got', 'accustomed', 'to', 'the', 'high', 'levels', 'of', 'graphic', 'violence', 'not', 'just', 'violence', 'but', 'injustice', 'crooked', 'guards', 'wholl', 'be', 'sold', 'out', 'for', 'a', 'nickel', 'inmates', 'wholl', 'kill', 'on', 'order', 'and', 'get', 'away', 'with', 'it', 'well', 'mannered', 'middle', 'class', 'inmates', 'being', 'turned', 'into', 'prison', 'bitches', 'due', 'to', 'their', 'lack', 'of', 'street', 'skills', 'or', 'prison', 'experience', 'watching', 'oz', 'you', 'may', 'become', 'comfortable', 'with', 'what', 'is', 'uncomfortable', 'viewingthats', 'if', 'you', 'can', 'get', 'in', 'touch', 'with', 'your', 'darker', 'side']]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Split Sentences\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    sentences = text.split(\".\")\n",
    "    sentences = [sentence.split() for sentence in sentences if sentence]\n",
    "     # Strip whitespace from each word  \n",
    "    sentences = [[word.strip() for word in sentence] for sentence in sentences]\n",
    "    return sentences\n",
    "\n",
    "sentences = preprocess_text(text)\n",
    "print(\"Sentences:\", sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80ede80",
   "metadata": {},
   "source": [
    "## Step 2: Build Vocabulary\n",
    "\n",
    "We will create a vocabulary of unique words and map each word to an index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb93d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word to Index: {'they': 0, 'what': 1, 'gangstas': 2, 'an': 3, 'their': 4, 'muslims': 5, 'face': 6, 'being': 7, 'get': 8, 'to': 9, 'exactly': 10, 'due': 11, 'word': 12, 'painted': 13, 'youll': 14, 'accustomed': 15, 'just': 16, 'after': 17, 'from': 18, 'hooked': 19, 'focuses': 20, 'italians': 21, 'guards': 22, 'thing': 23, 'oz': 24, 'watched': 25, 'levels': 26, 'has': 27, 'trust': 28, 'all': 29, 'well': 30, 'surreal': 31, 'viewingthats': 32, 'given': 33, 'use': 34, 'which': 35, 'of': 36, 'show': 37, 'stares': 38, 'me': 39, 'the': 40, 'city': 41, 'one': 42, 'a': 43, 'not': 44, 'em': 45, 'struck': 46, 'wordbr': 47, 'br': 48, 'so': 49, 'called': 50, 'injustice': 51, 'side': 52, 'graphic': 53, 'experience': 54, 'oswald': 55, 'high': 56, 'moreso': 57, 'ever': 58, 'dodgy': 59, 'goes': 60, 'wouldnt': 61, 'nasty': 62, 'its': 63, 'i': 64, 'doesnt': 65, 'latinos': 66, 'would': 67, 'scenes': 68, 'middle': 69, 'comfortable': 70, 'timid': 71, 'skills': 72, 'where': 73, 'about': 74, 'glass': 75, 'around': 76, 'reviewers': 77, 'crooked': 78, 'turned': 79, 'saw': 80, 'as': 81, 'your': 82, 'agenda': 83, '1': 84, 'away': 85, 'on': 86, 'inwards': 87, 'taste': 88, 'be': 89, 'forget': 90, 'security': 91, 'uncomfortable': 92, 'mainly': 93, 'it': 94, 'order': 95, 'never': 96, 'fact': 97, 'dare': 98, 'mainstream': 99, 'mebr': 100, 'is': 101, 'unflinching': 102, 'street': 103, 'death': 104, 'couldnt': 105, 'mannered': 106, 'experimental': 107, 'charm': 108, 'first': 109, 'cells': 110, 'no': 111, 'shows': 112, 'nickel': 113, 'developed': 114, 'bitches': 115, 'you': 116, 'episode': 117, 'got': 118, 'ready': 119, 'was': 120, 'pretty': 121, 'can': 122, 'wholl': 123, 'punches': 124, 'drugs': 125, 'agreements': 126, 'penitentary': 127, 'class': 128, 'in': 129, 'happened': 130, 'section': 131, 'sex': 132, 'manyaryans': 133, 'romanceoz': 134, 'faint': 135, 'pictures': 136, 'state': 137, 'out': 138, 'this': 139, 'watching': 140, 'audiences': 141, 'into': 142, 'become': 143, 'touch': 144, 'christians': 145, 'irish': 146, 'classic': 147, 'lack': 148, 'hearted': 149, 'other': 150, 'mentioned': 151, 'with': 152, 'nickname': 153, 'home': 154, 'far': 155, 'violence': 156, 'maximum': 157, 'mess': 158, 'kill': 159, 'prison': 160, 'awaybr': 161, 'darker': 162, 'but': 163, 'dealings': 164, 'may': 165, 'emerald': 166, 'inmates': 167, 'have': 168, 'or': 169, 'and': 170, 'for': 171, 'scuffles': 172, 'fronts': 173, 'privacy': 174, 'regards': 175, 'main': 176, 'hardcore': 177, 'appeal': 178, 'brutality': 179, 'pulls': 180, 'set': 181, 'go': 182, 'right': 183, 'more': 184, 'shady': 185, 'say': 186, 'are': 187, 'sold': 188, 'if': 189, 'that': 190}\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Make Vocabulary\n",
    "def build_vocabulary(sentences):\n",
    "    vocabulary = set()\n",
    "    for sentence in sentences:\n",
    "        vocabulary.update(sentence)\n",
    "    word_to_index = {word: idx for idx, word in enumerate(vocabulary)}\n",
    "    index_to_word = {idx: word for word, idx in word_to_index.items()}\n",
    "    return word_to_index, index_to_word\n",
    "\n",
    "\n",
    "word_to_index, index_to_word = build_vocabulary(sentences)\n",
    "print(\"Word to Index:\", word_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60e0cb5",
   "metadata": {},
   "source": [
    "## Step 3: One-Hot Encoding\n",
    "\n",
    "Convert each word into a one-hot encoded vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03dabbb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot encoding for 'you': [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Import numpy\n",
    "import numpy as np\n",
    "# Step 3: One-Hot Encode\n",
    "def one_hot_encode(word, word_to_index):\n",
    "    word = word.strip()  # Strip whitespace from the word\n",
    "    vector = np.zeros(len(word_to_index))\n",
    "    vector[word_to_index[word]] = 1\n",
    "    return vector\n",
    "\n",
    "# Example of one-hot encoding\n",
    "example_word = \"you\"\n",
    "print(f\"One-hot encoding for '{example_word}':\", one_hot_encode(example_word, word_to_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4470b0",
   "metadata": {},
   "source": [
    "## Step 4: Prepare Training Data\n",
    "\n",
    "Generate training pairs of a target word and its surrounding context words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1d4d7db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data: [(['of', 'the'], 'one'), (['one', 'the', 'other'], 'of'), (['one', 'of', 'other', 'reviewers'], 'the'), (['of', 'the', 'reviewers', 'has'], 'other'), (['the', 'other', 'has', 'mentioned'], 'reviewers'), (['other', 'reviewers', 'mentioned', 'that'], 'has'), (['reviewers', 'has', 'that', 'after'], 'mentioned'), (['has', 'mentioned', 'after', 'watching'], 'that'), (['mentioned', 'that', 'watching', 'just'], 'after'), (['that', 'after', 'just', '1'], 'watching'), (['after', 'watching', '1', 'oz'], 'just'), (['watching', 'just', 'oz', 'episode'], '1'), (['just', '1', 'episode', 'youll'], 'oz'), (['1', 'oz', 'youll', 'be'], 'episode'), (['oz', 'episode', 'be', 'hooked'], 'youll'), (['episode', 'youll', 'hooked', 'they'], 'be'), (['youll', 'be', 'they', 'are'], 'hooked'), (['be', 'hooked', 'are', 'right'], 'they'), (['hooked', 'they', 'right', 'as'], 'are'), (['they', 'are', 'as', 'this'], 'right'), (['are', 'right', 'this', 'is'], 'as'), (['right', 'as', 'is', 'exactly'], 'this'), (['as', 'this', 'exactly', 'what'], 'is'), (['this', 'is', 'what', 'happened'], 'exactly'), (['is', 'exactly', 'happened', 'with'], 'what'), (['exactly', 'what', 'with', 'mebr'], 'happened'), (['what', 'happened', 'mebr', 'br'], 'with'), (['happened', 'with', 'br', 'the'], 'mebr'), (['with', 'mebr', 'the', 'first'], 'br'), (['mebr', 'br', 'first', 'thing'], 'the'), (['br', 'the', 'thing', 'that'], 'first'), (['the', 'first', 'that', 'struck'], 'thing'), (['first', 'thing', 'struck', 'me'], 'that'), (['thing', 'that', 'me', 'about'], 'struck'), (['that', 'struck', 'about', 'oz'], 'me'), (['struck', 'me', 'oz', 'was'], 'about'), (['me', 'about', 'was', 'its'], 'oz'), (['about', 'oz', 'its', 'brutality'], 'was'), (['oz', 'was', 'brutality', 'and'], 'its'), (['was', 'its', 'and', 'unflinching'], 'brutality'), (['its', 'brutality', 'unflinching', 'scenes'], 'and'), (['brutality', 'and', 'scenes', 'of'], 'unflinching'), (['and', 'unflinching', 'of', 'violence'], 'scenes'), (['unflinching', 'scenes', 'violence', 'which'], 'of'), (['scenes', 'of', 'which', 'set'], 'violence'), (['of', 'violence', 'set', 'in'], 'which'), (['violence', 'which', 'in', 'right'], 'set'), (['which', 'set', 'right', 'from'], 'in'), (['set', 'in', 'from', 'the'], 'right'), (['in', 'right', 'the', 'word'], 'from'), (['right', 'from', 'word', 'go'], 'the'), (['from', 'the', 'go', 'trust'], 'word'), (['the', 'word', 'trust', 'me'], 'go'), (['word', 'go', 'me', 'this'], 'trust'), (['go', 'trust', 'this', 'is'], 'me'), (['trust', 'me', 'is', 'not'], 'this'), (['me', 'this', 'not', 'a'], 'is'), (['this', 'is', 'a', 'show'], 'not'), (['is', 'not', 'show', 'for'], 'a'), (['not', 'a', 'for', 'the'], 'show'), (['a', 'show', 'the', 'faint'], 'for'), (['show', 'for', 'faint', 'hearted'], 'the'), (['for', 'the', 'hearted', 'or'], 'faint'), (['the', 'faint', 'or', 'timid'], 'hearted'), (['faint', 'hearted', 'timid', 'this'], 'or'), (['hearted', 'or', 'this', 'show'], 'timid'), (['or', 'timid', 'show', 'pulls'], 'this'), (['timid', 'this', 'pulls', 'no'], 'show'), (['this', 'show', 'no', 'punches'], 'pulls'), (['show', 'pulls', 'punches', 'with'], 'no'), (['pulls', 'no', 'with', 'regards'], 'punches'), (['no', 'punches', 'regards', 'to'], 'with'), (['punches', 'with', 'to', 'drugs'], 'regards'), (['with', 'regards', 'drugs', 'sex'], 'to'), (['regards', 'to', 'sex', 'or'], 'drugs'), (['to', 'drugs', 'or', 'violence'], 'sex'), (['drugs', 'sex', 'violence', 'its'], 'or'), (['sex', 'or', 'its', 'is'], 'violence'), (['or', 'violence', 'is', 'hardcore'], 'its'), (['violence', 'its', 'hardcore', 'in'], 'is'), (['its', 'is', 'in', 'the'], 'hardcore'), (['is', 'hardcore', 'the', 'classic'], 'in'), (['hardcore', 'in', 'classic', 'use'], 'the'), (['in', 'the', 'use', 'of'], 'classic'), (['the', 'classic', 'of', 'the'], 'use'), (['classic', 'use', 'the', 'wordbr'], 'of'), (['use', 'of', 'wordbr', 'br'], 'the'), (['of', 'the', 'br', 'it'], 'wordbr'), (['the', 'wordbr', 'it', 'is'], 'br'), (['wordbr', 'br', 'is', 'called'], 'it'), (['br', 'it', 'called', 'oz'], 'is'), (['it', 'is', 'oz', 'as'], 'called'), (['is', 'called', 'as', 'that'], 'oz'), (['called', 'oz', 'that', 'is'], 'as'), (['oz', 'as', 'is', 'the'], 'that'), (['as', 'that', 'the', 'nickname'], 'is'), (['that', 'is', 'nickname', 'given'], 'the'), (['is', 'the', 'given', 'to'], 'nickname'), (['the', 'nickname', 'to', 'the'], 'given'), (['nickname', 'given', 'the', 'oswald'], 'to'), (['given', 'to', 'oswald', 'maximum'], 'the'), (['to', 'the', 'maximum', 'security'], 'oswald'), (['the', 'oswald', 'security', 'state'], 'maximum'), (['oswald', 'maximum', 'state', 'penitentary'], 'security'), (['maximum', 'security', 'penitentary', 'it'], 'state'), (['security', 'state', 'it', 'focuses'], 'penitentary'), (['state', 'penitentary', 'focuses', 'mainly'], 'it'), (['penitentary', 'it', 'mainly', 'on'], 'focuses'), (['it', 'focuses', 'on', 'emerald'], 'mainly'), (['focuses', 'mainly', 'emerald', 'city'], 'on'), (['mainly', 'on', 'city', 'an'], 'emerald'), (['on', 'emerald', 'an', 'experimental'], 'city'), (['emerald', 'city', 'experimental', 'section'], 'an'), (['city', 'an', 'section', 'of'], 'experimental'), (['an', 'experimental', 'of', 'the'], 'section'), (['experimental', 'section', 'the', 'prison'], 'of'), (['section', 'of', 'prison', 'where'], 'the'), (['of', 'the', 'where', 'all'], 'prison'), (['the', 'prison', 'all', 'the'], 'where'), (['prison', 'where', 'the', 'cells'], 'all'), (['where', 'all', 'cells', 'have'], 'the'), (['all', 'the', 'have', 'glass'], 'cells'), (['the', 'cells', 'glass', 'fronts'], 'have'), (['cells', 'have', 'fronts', 'and'], 'glass'), (['have', 'glass', 'and', 'face'], 'fronts'), (['glass', 'fronts', 'face', 'inwards'], 'and'), (['fronts', 'and', 'inwards', 'so'], 'face'), (['and', 'face', 'so', 'privacy'], 'inwards'), (['face', 'inwards', 'privacy', 'is'], 'so'), (['inwards', 'so', 'is', 'not'], 'privacy'), (['so', 'privacy', 'not', 'high'], 'is'), (['privacy', 'is', 'high', 'on'], 'not'), (['is', 'not', 'on', 'the'], 'high'), (['not', 'high', 'the', 'agenda'], 'on'), (['high', 'on', 'agenda', 'em'], 'the'), (['on', 'the', 'em', 'city'], 'agenda'), (['the', 'agenda', 'city', 'is'], 'em'), (['agenda', 'em', 'is', 'home'], 'city'), (['em', 'city', 'home', 'to'], 'is'), (['city', 'is', 'to', 'manyaryans'], 'home'), (['is', 'home', 'manyaryans', 'muslims'], 'to'), (['home', 'to', 'muslims', 'gangstas'], 'manyaryans'), (['to', 'manyaryans', 'gangstas', 'latinos'], 'muslims'), (['manyaryans', 'muslims', 'latinos', 'christians'], 'gangstas'), (['muslims', 'gangstas', 'christians', 'italians'], 'latinos'), (['gangstas', 'latinos', 'italians', 'irish'], 'christians'), (['latinos', 'christians', 'irish', 'and'], 'italians'), (['christians', 'italians', 'and', 'moreso'], 'irish'), (['italians', 'irish', 'moreso', 'scuffles'], 'and'), (['irish', 'and', 'scuffles', 'death'], 'moreso'), (['and', 'moreso', 'death', 'stares'], 'scuffles'), (['moreso', 'scuffles', 'stares', 'dodgy'], 'death'), (['scuffles', 'death', 'dodgy', 'dealings'], 'stares'), (['death', 'stares', 'dealings', 'and'], 'dodgy'), (['stares', 'dodgy', 'and', 'shady'], 'dealings'), (['dodgy', 'dealings', 'shady', 'agreements'], 'and'), (['dealings', 'and', 'agreements', 'are'], 'shady'), (['and', 'shady', 'are', 'never'], 'agreements'), (['shady', 'agreements', 'never', 'far'], 'are'), (['agreements', 'are', 'far', 'awaybr'], 'never'), (['are', 'never', 'awaybr', 'br'], 'far'), (['never', 'far', 'br', 'i'], 'awaybr'), (['far', 'awaybr', 'i', 'would'], 'br'), (['awaybr', 'br', 'would', 'say'], 'i'), (['br', 'i', 'say', 'the'], 'would'), (['i', 'would', 'the', 'main'], 'say'), (['would', 'say', 'main', 'appeal'], 'the'), (['say', 'the', 'appeal', 'of'], 'main'), (['the', 'main', 'of', 'the'], 'appeal'), (['main', 'appeal', 'the', 'show'], 'of'), (['appeal', 'of', 'show', 'is'], 'the'), (['of', 'the', 'is', 'due'], 'show'), (['the', 'show', 'due', 'to'], 'is'), (['show', 'is', 'to', 'the'], 'due'), (['is', 'due', 'the', 'fact'], 'to'), (['due', 'to', 'fact', 'that'], 'the'), (['to', 'the', 'that', 'it'], 'fact'), (['the', 'fact', 'it', 'goes'], 'that'), (['fact', 'that', 'goes', 'where'], 'it'), (['that', 'it', 'where', 'other'], 'goes'), (['it', 'goes', 'other', 'shows'], 'where'), (['goes', 'where', 'shows', 'wouldnt'], 'other'), (['where', 'other', 'wouldnt', 'dare'], 'shows'), (['other', 'shows', 'dare', 'forget'], 'wouldnt'), (['shows', 'wouldnt', 'forget', 'pretty'], 'dare'), (['wouldnt', 'dare', 'pretty', 'pictures'], 'forget'), (['dare', 'forget', 'pictures', 'painted'], 'pretty'), (['forget', 'pretty', 'painted', 'for'], 'pictures'), (['pretty', 'pictures', 'for', 'mainstream'], 'painted'), (['pictures', 'painted', 'mainstream', 'audiences'], 'for'), (['painted', 'for', 'audiences', 'forget'], 'mainstream'), (['for', 'mainstream', 'forget', 'charm'], 'audiences'), (['mainstream', 'audiences', 'charm', 'forget'], 'forget'), (['audiences', 'forget', 'forget', 'romanceoz'], 'charm'), (['forget', 'charm', 'romanceoz', 'doesnt'], 'forget'), (['charm', 'forget', 'doesnt', 'mess'], 'romanceoz'), (['forget', 'romanceoz', 'mess', 'around'], 'doesnt'), (['romanceoz', 'doesnt', 'around', 'the'], 'mess'), (['doesnt', 'mess', 'the', 'first'], 'around'), (['mess', 'around', 'first', 'episode'], 'the'), (['around', 'the', 'episode', 'i'], 'first'), (['the', 'first', 'i', 'ever'], 'episode'), (['first', 'episode', 'ever', 'saw'], 'i'), (['episode', 'i', 'saw', 'struck'], 'ever'), (['i', 'ever', 'struck', 'me'], 'saw'), (['ever', 'saw', 'me', 'as'], 'struck'), (['saw', 'struck', 'as', 'so'], 'me'), (['struck', 'me', 'so', 'nasty'], 'as'), (['me', 'as', 'nasty', 'it'], 'so'), (['as', 'so', 'it', 'was'], 'nasty'), (['so', 'nasty', 'was', 'surreal'], 'it'), (['nasty', 'it', 'surreal', 'i'], 'was'), (['it', 'was', 'i', 'couldnt'], 'surreal'), (['was', 'surreal', 'couldnt', 'say'], 'i'), (['surreal', 'i', 'say', 'i'], 'couldnt'), (['i', 'couldnt', 'i', 'was'], 'say'), (['couldnt', 'say', 'was', 'ready'], 'i'), (['say', 'i', 'ready', 'for'], 'was'), (['i', 'was', 'for', 'it'], 'ready'), (['was', 'ready', 'it', 'but'], 'for'), (['ready', 'for', 'but', 'as'], 'it'), (['for', 'it', 'as', 'i'], 'but'), (['it', 'but', 'i', 'watched'], 'as'), (['but', 'as', 'watched', 'more'], 'i'), (['as', 'i', 'more', 'i'], 'watched'), (['i', 'watched', 'i', 'developed'], 'more'), (['watched', 'more', 'developed', 'a'], 'i'), (['more', 'i', 'a', 'taste'], 'developed'), (['i', 'developed', 'taste', 'for'], 'a'), (['developed', 'a', 'for', 'oz'], 'taste'), (['a', 'taste', 'oz', 'and'], 'for'), (['taste', 'for', 'and', 'got'], 'oz'), (['for', 'oz', 'got', 'accustomed'], 'and'), (['oz', 'and', 'accustomed', 'to'], 'got'), (['and', 'got', 'to', 'the'], 'accustomed'), (['got', 'accustomed', 'the', 'high'], 'to'), (['accustomed', 'to', 'high', 'levels'], 'the'), (['to', 'the', 'levels', 'of'], 'high'), (['the', 'high', 'of', 'graphic'], 'levels'), (['high', 'levels', 'graphic', 'violence'], 'of'), (['levels', 'of', 'violence', 'not'], 'graphic'), (['of', 'graphic', 'not', 'just'], 'violence'), (['graphic', 'violence', 'just', 'violence'], 'not'), (['violence', 'not', 'violence', 'but'], 'just'), (['not', 'just', 'but', 'injustice'], 'violence'), (['just', 'violence', 'injustice', 'crooked'], 'but'), (['violence', 'but', 'crooked', 'guards'], 'injustice'), (['but', 'injustice', 'guards', 'wholl'], 'crooked'), (['injustice', 'crooked', 'wholl', 'be'], 'guards'), (['crooked', 'guards', 'be', 'sold'], 'wholl'), (['guards', 'wholl', 'sold', 'out'], 'be'), (['wholl', 'be', 'out', 'for'], 'sold'), (['be', 'sold', 'for', 'a'], 'out'), (['sold', 'out', 'a', 'nickel'], 'for'), (['out', 'for', 'nickel', 'inmates'], 'a'), (['for', 'a', 'inmates', 'wholl'], 'nickel'), (['a', 'nickel', 'wholl', 'kill'], 'inmates'), (['nickel', 'inmates', 'kill', 'on'], 'wholl'), (['inmates', 'wholl', 'on', 'order'], 'kill'), (['wholl', 'kill', 'order', 'and'], 'on'), (['kill', 'on', 'and', 'get'], 'order'), (['on', 'order', 'get', 'away'], 'and'), (['order', 'and', 'away', 'with'], 'get'), (['and', 'get', 'with', 'it'], 'away'), (['get', 'away', 'it', 'well'], 'with'), (['away', 'with', 'well', 'mannered'], 'it'), (['with', 'it', 'mannered', 'middle'], 'well'), (['it', 'well', 'middle', 'class'], 'mannered'), (['well', 'mannered', 'class', 'inmates'], 'middle'), (['mannered', 'middle', 'inmates', 'being'], 'class'), (['middle', 'class', 'being', 'turned'], 'inmates'), (['class', 'inmates', 'turned', 'into'], 'being'), (['inmates', 'being', 'into', 'prison'], 'turned'), (['being', 'turned', 'prison', 'bitches'], 'into'), (['turned', 'into', 'bitches', 'due'], 'prison'), (['into', 'prison', 'due', 'to'], 'bitches'), (['prison', 'bitches', 'to', 'their'], 'due'), (['bitches', 'due', 'their', 'lack'], 'to'), (['due', 'to', 'lack', 'of'], 'their'), (['to', 'their', 'of', 'street'], 'lack'), (['their', 'lack', 'street', 'skills'], 'of'), (['lack', 'of', 'skills', 'or'], 'street'), (['of', 'street', 'or', 'prison'], 'skills'), (['street', 'skills', 'prison', 'experience'], 'or'), (['skills', 'or', 'experience', 'watching'], 'prison'), (['or', 'prison', 'watching', 'oz'], 'experience'), (['prison', 'experience', 'oz', 'you'], 'watching'), (['experience', 'watching', 'you', 'may'], 'oz'), (['watching', 'oz', 'may', 'become'], 'you'), (['oz', 'you', 'become', 'comfortable'], 'may'), (['you', 'may', 'comfortable', 'with'], 'become'), (['may', 'become', 'with', 'what'], 'comfortable'), (['become', 'comfortable', 'what', 'is'], 'with'), (['comfortable', 'with', 'is', 'uncomfortable'], 'what'), (['with', 'what', 'uncomfortable', 'viewingthats'], 'is'), (['what', 'is', 'viewingthats', 'if'], 'uncomfortable'), (['is', 'uncomfortable', 'if', 'you'], 'viewingthats'), (['uncomfortable', 'viewingthats', 'you', 'can'], 'if'), (['viewingthats', 'if', 'can', 'get'], 'you'), (['if', 'you', 'get', 'in'], 'can'), (['you', 'can', 'in', 'touch'], 'get'), (['can', 'get', 'touch', 'with'], 'in'), (['get', 'in', 'with', 'your'], 'touch'), (['in', 'touch', 'your', 'darker'], 'with'), (['touch', 'with', 'darker', 'side'], 'your'), (['with', 'your', 'side'], 'darker'), (['your', 'darker'], 'side')]\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Prepare Training Data\n",
    "def generate_training_data(sentences, word_to_index, window_size=2):\n",
    "    training_data = []\n",
    "    for sentence in sentences:\n",
    "        for i, target_word in enumerate(sentence):\n",
    "            context = []\n",
    "            for j in range(-window_size, window_size + 1):\n",
    "                if j != 0 and 0 <= i + j < len(sentence):\n",
    "                    context.append(sentence[i + j])\n",
    "            training_data.append((context, target_word))\n",
    "    return training_data\n",
    "\n",
    "training_data = generate_training_data(sentences, word_to_index)\n",
    "print(\"Training Data:\", training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d4d175",
   "metadata": {},
   "source": [
    "## Step 5: Initialize Weights\n",
    "\n",
    "Randomly initialize weights for the neural network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ec25efd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 Shape: (191, 10)\n",
      "W2 Shape: (10, 191)\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Initialize Weights\n",
    "def initialize_weights(vocab_size, embedding_dim):\n",
    "    W1 = np.random.rand(vocab_size, embedding_dim)\n",
    "    W2 = np.random.rand(embedding_dim, vocab_size)\n",
    "    return W1, W2\n",
    "\n",
    "vocab_size = len(word_to_index)\n",
    "embedding_dim = 10\n",
    "W1, W2 = initialize_weights(vocab_size, embedding_dim)\n",
    "print(\"W1 Shape:\", W1.shape)\n",
    "print(\"W2 Shape:\", W2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8800b6a8",
   "metadata": {},
   "source": [
    "## Step 6: Forward Pass\n",
    "\n",
    "Use the context words to predict the target word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e248759e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [0.00213227 0.0018785  0.00332152 0.0006865  0.00435646 0.00288194\n",
      " 0.00205401 0.00560074 0.00597857 0.00726922 0.00270178 0.00111142\n",
      " 0.00446999 0.0156748  0.01047186 0.00188084 0.00351845 0.01132483\n",
      " 0.00132852 0.0010822  0.00131235 0.00505399 0.0089736  0.00641337\n",
      " 0.00066024 0.00304097 0.00466539 0.00278789 0.00847183 0.00200756\n",
      " 0.00055433 0.00118528 0.00182418 0.00737007 0.00423864 0.00783332\n",
      " 0.01147866 0.00787929 0.00770173 0.01345859 0.00961481 0.00474966\n",
      " 0.00270046 0.00309687 0.00264362 0.00112812 0.00552614 0.00603089\n",
      " 0.00358825 0.0036818  0.00480967 0.01083732 0.00416332 0.00392931\n",
      " 0.001359   0.00147904 0.00895109 0.00276624 0.00212335 0.00308182\n",
      " 0.00721263 0.00174748 0.00468638 0.00159617 0.00201591 0.00163697\n",
      " 0.00354256 0.0029527  0.00215963 0.00306316 0.01441095 0.00168572\n",
      " 0.00824243 0.00377058 0.00407944 0.00370662 0.00195967 0.00925811\n",
      " 0.00193402 0.0022038  0.00144293 0.02957747 0.00609281 0.01069854\n",
      " 0.00723083 0.00259711 0.01888807 0.00432152 0.01241849 0.0044355\n",
      " 0.00169306 0.00778518 0.00231106 0.00227198 0.00183118 0.0082492\n",
      " 0.00431517 0.00264608 0.00441978 0.00135917 0.00247259 0.01366276\n",
      " 0.00163038 0.00251037 0.00519396 0.00132915 0.00323959 0.00216429\n",
      " 0.00059178 0.00601822 0.00420176 0.00421849 0.01318826 0.00182141\n",
      " 0.0013874  0.00066111 0.0027843  0.00120496 0.01414563 0.00978753\n",
      " 0.00261079 0.00803738 0.00330611 0.0031631  0.03181087 0.00320374\n",
      " 0.03093152 0.00288029 0.00245565 0.00136941 0.01370203 0.00307855\n",
      " 0.00692689 0.00179675 0.00903739 0.00377347 0.00395957 0.00096728\n",
      " 0.00259798 0.0005054  0.00108712 0.0026518  0.00423315 0.00236788\n",
      " 0.01087528 0.00408206 0.0070139  0.01164833 0.01180849 0.00020233\n",
      " 0.00326589 0.01081858 0.01233249 0.00171252 0.00389223 0.00953077\n",
      " 0.00261469 0.00463821 0.0197805  0.00221115 0.00236446 0.00290894\n",
      " 0.00222325 0.00181861 0.00225408 0.01251368 0.00394678 0.00597447\n",
      " 0.00372338 0.00271841 0.00171366 0.00260813 0.00923567 0.00089698\n",
      " 0.00178421 0.00371426 0.0005756  0.0097419  0.00575315 0.01098743\n",
      " 0.00607272 0.00158602 0.00899264 0.00494527 0.00273521 0.00841709\n",
      " 0.00791601 0.00268704 0.00501841 0.00835838 0.0013721 ]\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Forward Pass\n",
    "def forward_pass(context_words, W1, W2, word_to_index):\n",
    "    context_vectors = np.sum([one_hot_encode(word, word_to_index) for word in context_words], axis=0)\n",
    "    hidden_layer = np.dot(context_vectors, W1)\n",
    "    output_layer = np.dot(hidden_layer, W2)\n",
    "    predictions = softmax(output_layer)\n",
    "    return predictions, hidden_layer\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x))\n",
    "    return exp_x / exp_x.sum(axis=0)\n",
    "\n",
    "# Example forward pass\n",
    "context_words = [\"the\", \"you\"]\n",
    "predictions, hidden_layer = forward_pass(context_words, W1, W2, word_to_index)\n",
    "print(\"Predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44487481",
   "metadata": {},
   "source": [
    "## Step 7: Calculate Loss\n",
    "\n",
    "Compute the loss to measure how far the predictions are from the actual target.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7ae771eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 5.8837579364215795\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Calculate Loss\n",
    "def calculate_loss(predictions, target_word, word_to_index):\n",
    "    target_vector = one_hot_encode(target_word, word_to_index)\n",
    "    loss = -np.sum(target_vector * np.log(predictions))\n",
    "    return loss\n",
    "\n",
    "# Example loss calculation\n",
    "target_word = \"you\"\n",
    "loss = calculate_loss(predictions, target_word, word_to_index)\n",
    "print(\"Loss:\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a178af7",
   "metadata": {},
   "source": [
    "## Step 8: Update Weights\n",
    "\n",
    "Adjust the weights using backpropagation to minimize the loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "83bb473b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated W1: [[0.08310851 0.74618342 0.19801752 ... 0.40738392 0.35007402 0.77353624]\n",
      " [0.76821327 0.86323211 0.21154912 ... 0.17984746 0.16831906 0.32361618]\n",
      " [0.45822773 0.00705465 0.20741367 ... 0.20689923 0.13580686 0.56027178]\n",
      " ...\n",
      " [0.06330716 0.08635212 0.73143845 ... 0.3129106  0.29609666 0.40056424]\n",
      " [0.66156429 0.49835694 0.97768569 ... 0.52090851 0.06955654 0.31915977]\n",
      " [0.5397931  0.20921538 0.19101099 ... 0.42257942 0.39824093 0.94691191]]\n",
      "Updated W2: [[0.83090592 0.65196339 0.52216267 ... 0.32916463 0.58848774 0.57794898]\n",
      " [0.21378237 0.87540133 0.0203004  ... 0.9942027  0.33454198 0.71350138]\n",
      " [0.09581668 0.00745751 0.81211325 ... 0.12431117 0.38603117 0.17427445]\n",
      " ...\n",
      " [0.87991565 0.37731752 0.74163782 ... 0.08974289 0.02208898 0.41896346]\n",
      " [0.91831192 0.83277559 0.83246297 ... 0.12639498 0.77119136 0.10360173]\n",
      " [0.78077818 0.00524953 0.76746312 ... 0.49719124 0.66159015 0.73420595]]\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Update Weights\n",
    "def backpropagate(W1, W2, hidden_layer, context_words, predictions, target_word, word_to_index, learning_rate=0.01):\n",
    "    target_vector = one_hot_encode(target_word, word_to_index)\n",
    "    error = predictions - target_vector\n",
    "    dW2 = np.outer(hidden_layer, error)\n",
    "    dW1 = np.outer(np.sum([one_hot_encode(word, word_to_index) for word in context_words], axis=0), np.dot(W2, error))\n",
    "    W1 -= learning_rate * dW1\n",
    "    W2 -= learning_rate * dW2\n",
    "    return W1, W2\n",
    "\n",
    "# Example weight update\n",
    "W1, W2 = backpropagate(W1, W2, hidden_layer, context_words, predictions, target_word, word_to_index)\n",
    "print(\"Updated W1:\", W1)\n",
    "print(\"Updated W2:\", W2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22ce2ad",
   "metadata": {},
   "source": [
    "## Training the CBOW Model\n",
    "\n",
    "We will train the CBOW model on the example text corpus for multiple epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "43a9bbd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Loss: 1876.6766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200, Loss: 153.2660\n",
      "Epoch 300, Loss: 26.4190\n",
      "Epoch 400, Loss: 12.0695\n",
      "Epoch 500, Loss: 7.4540\n",
      "Epoch 600, Loss: 5.2811\n",
      "Epoch 700, Loss: 4.0427\n",
      "Epoch 800, Loss: 3.2513\n",
      "Epoch 900, Loss: 2.7058\n",
      "Epoch 1000, Loss: 2.3088\n"
     ]
    }
   ],
   "source": [
    "# Training the CBOW Model\n",
    "for epoch in range(1000):\n",
    "    total_loss = 0\n",
    "    for context_words, target_word in training_data:\n",
    "        predictions, hidden_layer = forward_pass(context_words, W1, W2, word_to_index)\n",
    "        loss = calculate_loss(predictions, target_word, word_to_index)\n",
    "        total_loss += loss\n",
    "        W1, W2 = backpropagate(W1, W2, hidden_layer, context_words, predictions, target_word, word_to_index)\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch+100}, Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "06a256dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000, Loss: 2.0080\n",
      "Epoch 2000, Loss: 0.8353\n",
      "Epoch 3000, Loss: 0.5118\n",
      "Epoch 4000, Loss: 0.3640\n",
      "Epoch 5000, Loss: 0.2803\n",
      "Epoch 6000, Loss: 0.2268\n",
      "Epoch 7000, Loss: 0.1899\n",
      "Epoch 8000, Loss: 0.1628\n",
      "Epoch 9000, Loss: 0.1423\n",
      "Epoch 10000, Loss: 0.1262\n"
     ]
    }
   ],
   "source": [
    "# Training the CBOW Model\n",
    "for epoch in range(10000):\n",
    "    total_loss = 0\n",
    "    for context_words, target_word in training_data:\n",
    "        predictions, hidden_layer = forward_pass(context_words, W1, W2, word_to_index)\n",
    "        loss = calculate_loss(predictions, target_word, word_to_index)\n",
    "        total_loss += loss\n",
    "        W1, W2 = backpropagate(W1, W2, hidden_layer, context_words, predictions, target_word, word_to_index)\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch+1000}, Loss: {total_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
