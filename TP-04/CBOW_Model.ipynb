{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bfc76be",
   "metadata": {},
   "source": [
    "## Step 1: Preprocess Text\n",
    "\n",
    "We will preprocess the text by converting it to lowercase, removing punctuation, and splitting it into sentences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6b9f613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences: [['do', 'all', 'the', 'good', 'you', 'can', 'for', 'all', 'the', 'people', 'you', 'can', 'in', 'all', 'the', 'ways', 'you', 'can', 'as', 'long', 'as', 'you', 'can']]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "# Step 1: Split Sentences\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    sentences = text.split(\".\")\n",
    "    sentences = [sentence.split() for sentence in sentences if sentence]\n",
    "    return sentences\n",
    "\n",
    "# Example text\n",
    "text = \"Do all the good you can, for all the people you can, in all the ways you can, as long as you can.\"\n",
    "sentences = preprocess_text(text)\n",
    "print(\"Sentences:\", sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80ede80",
   "metadata": {},
   "source": [
    "## Step 2: Build Vocabulary\n",
    "\n",
    "We will create a vocabulary of unique words and map each word to an index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fb93d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word to Index: {'all': 0, 'ways': 1, 'the': 2, 'can': 3, 'in': 4, 'as': 5, 'long': 6, 'people': 7, 'for': 8, 'do': 9, 'you': 10, 'good': 11}\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Make Vocabulary\n",
    "def build_vocabulary(sentences):\n",
    "    vocabulary = set()\n",
    "    for sentence in sentences:\n",
    "        vocabulary.update(sentence)\n",
    "    word_to_index = {word: idx for idx, word in enumerate(vocabulary)}\n",
    "    index_to_word = {idx: word for word, idx in word_to_index.items()}\n",
    "    return word_to_index, index_to_word\n",
    "\n",
    "word_to_index, index_to_word = build_vocabulary(sentences)\n",
    "print(\"Word to Index:\", word_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60e0cb5",
   "metadata": {},
   "source": [
    "## Step 3: One-Hot Encoding\n",
    "\n",
    "Convert each word into a one-hot encoded vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03dabbb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot encoding for 'you': [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Step 3: One-Hot Encode\n",
    "def one_hot_encode(word, word_to_index):\n",
    "    vector = np.zeros(len(word_to_index))\n",
    "    vector[word_to_index[word]] = 1\n",
    "    return vector\n",
    "\n",
    "# Example of one-hot encoding\n",
    "example_word = \"you\"\n",
    "print(f\"One-hot encoding for '{example_word}':\", one_hot_encode(example_word, word_to_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4470b0",
   "metadata": {},
   "source": [
    "## Step 4: Prepare Training Data\n",
    "\n",
    "Generate training pairs of a target word and its surrounding context words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d4d7db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data: [(['all', 'the'], 'do'), (['do', 'the', 'good'], 'all'), (['do', 'all', 'good', 'you'], 'the'), (['all', 'the', 'you', 'can'], 'good'), (['the', 'good', 'can', 'for'], 'you'), (['good', 'you', 'for', 'all'], 'can'), (['you', 'can', 'all', 'the'], 'for'), (['can', 'for', 'the', 'people'], 'all'), (['for', 'all', 'people', 'you'], 'the'), (['all', 'the', 'you', 'can'], 'people'), (['the', 'people', 'can', 'in'], 'you'), (['people', 'you', 'in', 'all'], 'can'), (['you', 'can', 'all', 'the'], 'in'), (['can', 'in', 'the', 'ways'], 'all'), (['in', 'all', 'ways', 'you'], 'the'), (['all', 'the', 'you', 'can'], 'ways'), (['the', 'ways', 'can', 'as'], 'you'), (['ways', 'you', 'as', 'long'], 'can'), (['you', 'can', 'long', 'as'], 'as'), (['can', 'as', 'as', 'you'], 'long'), (['as', 'long', 'you', 'can'], 'as'), (['long', 'as', 'can'], 'you'), (['as', 'you'], 'can')]\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Prepare Training Data\n",
    "def generate_training_data(sentences, word_to_index, window_size=2):\n",
    "    training_data = []\n",
    "    for sentence in sentences:\n",
    "        for i, target_word in enumerate(sentence):\n",
    "            context = []\n",
    "            for j in range(-window_size, window_size + 1):\n",
    "                if j != 0 and 0 <= i + j < len(sentence):\n",
    "                    context.append(sentence[i + j])\n",
    "            training_data.append((context, target_word))\n",
    "    return training_data\n",
    "\n",
    "training_data = generate_training_data(sentences, word_to_index)\n",
    "print(\"Training Data:\", training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d4d175",
   "metadata": {},
   "source": [
    "## Step 5: Initialize Weights\n",
    "\n",
    "Randomly initialize weights for the neural network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec25efd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 Shape: (12, 10)\n",
      "W2 Shape: (10, 12)\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Initialize Weights\n",
    "def initialize_weights(vocab_size, embedding_dim):\n",
    "    W1 = np.random.rand(vocab_size, embedding_dim)\n",
    "    W2 = np.random.rand(embedding_dim, vocab_size)\n",
    "    return W1, W2\n",
    "\n",
    "vocab_size = len(word_to_index)\n",
    "embedding_dim = 10\n",
    "W1, W2 = initialize_weights(vocab_size, embedding_dim)\n",
    "print(\"W1 Shape:\", W1.shape)\n",
    "print(\"W2 Shape:\", W2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8800b6a8",
   "metadata": {},
   "source": [
    "## Step 6: Forward Pass\n",
    "\n",
    "Use the context words to predict the target word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e248759e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [0.1064354  0.03189628 0.09234783 0.0145338  0.05194879 0.05719299\n",
      " 0.0235437  0.31184857 0.06356431 0.04207062 0.16598507 0.03863265]\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Forward Pass\n",
    "def forward_pass(context_words, W1, W2, word_to_index):\n",
    "    context_vectors = np.sum([one_hot_encode(word, word_to_index) for word in context_words], axis=0)\n",
    "    hidden_layer = np.dot(context_vectors, W1)\n",
    "    output_layer = np.dot(hidden_layer, W2)\n",
    "    predictions = softmax(output_layer)\n",
    "    return predictions, hidden_layer\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x))\n",
    "    return exp_x / exp_x.sum(axis=0)\n",
    "\n",
    "# Example forward pass\n",
    "context_words = [\"the\", \"you\"]\n",
    "predictions, hidden_layer = forward_pass(context_words, W1, W2, word_to_index)\n",
    "print(\"Predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44487481",
   "metadata": {},
   "source": [
    "## Step 7: Calculate Loss\n",
    "\n",
    "Compute the loss to measure how far the predictions are from the actual target.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ae771eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.1652375598773608\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Calculate Loss\n",
    "def calculate_loss(predictions, target_word, word_to_index):\n",
    "    target_vector = one_hot_encode(target_word, word_to_index)\n",
    "    loss = -np.sum(target_vector * np.log(predictions))\n",
    "    return loss\n",
    "\n",
    "# Example loss calculation\n",
    "target_word = \"people\"\n",
    "loss = calculate_loss(predictions, target_word, word_to_index)\n",
    "print(\"Loss:\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a178af7",
   "metadata": {},
   "source": [
    "## Step 8: Update Weights\n",
    "\n",
    "Adjust the weights using backpropagation to minimize the loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83bb473b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated W1: [[8.66833928e-01 3.03421037e-01 7.68318448e-01 6.41010692e-01\n",
      "  6.41322655e-01 3.57514393e-01 8.23384213e-02 6.97104547e-01\n",
      "  7.87357177e-01 5.89386297e-01]\n",
      " [8.12133911e-01 8.68827777e-01 4.04661078e-01 2.43038845e-01\n",
      "  7.36329484e-01 8.20479664e-01 9.20181798e-01 6.97825512e-01\n",
      "  8.73882711e-01 1.90498343e-02]\n",
      " [1.59094675e-01 6.68858645e-01 4.70379115e-01 4.83024865e-01\n",
      "  7.73750775e-01 5.55570416e-01 7.36411009e-01 7.19100007e-01\n",
      "  1.68371542e-03 8.50767853e-01]\n",
      " [1.75309363e-02 5.48053815e-02 6.54673559e-01 3.80399396e-04\n",
      "  7.74482454e-01 4.64245582e-01 8.41407525e-01 2.15037160e-01\n",
      "  4.71566777e-01 9.73851496e-01]\n",
      " [3.59470800e-01 8.84070442e-01 7.05206425e-01 5.71526428e-01\n",
      "  4.37769582e-01 7.65634160e-01 8.44967976e-02 1.60965537e-01\n",
      "  4.57091873e-01 5.68071428e-01]\n",
      " [1.58690309e-01 7.14605948e-01 2.85556443e-01 7.98283872e-01\n",
      "  4.56681119e-01 9.85806779e-01 8.31624537e-03 8.41517722e-01\n",
      "  8.70994433e-01 9.30731618e-01]\n",
      " [2.87768865e-01 5.13340680e-01 1.62914511e-01 5.28514828e-01\n",
      "  1.03720139e-01 5.33096449e-01 8.27557083e-01 7.57205585e-01\n",
      "  4.49029742e-01 3.58527623e-01]\n",
      " [4.34270689e-01 9.78518979e-01 3.19054868e-01 7.55601122e-01\n",
      "  1.22016254e-01 4.80758179e-01 4.12374300e-01 8.22489341e-01\n",
      "  7.96250286e-01 3.83903515e-01]\n",
      " [4.23352708e-01 4.45569149e-01 1.07923244e-01 2.91238715e-01\n",
      "  5.28099367e-01 2.89259528e-01 7.67238824e-01 6.97483657e-01\n",
      "  6.12600853e-01 3.05432396e-01]\n",
      " [5.26126412e-01 7.69685927e-01 8.46283284e-01 8.22539543e-01\n",
      "  1.83422393e-01 9.81051589e-01 3.69943154e-01 2.98395185e-03\n",
      "  5.87455273e-01 2.25831879e-01]\n",
      " [6.56496575e-01 8.93805505e-01 8.31939362e-01 5.02647463e-01\n",
      "  2.93299760e-01 9.95527056e-01 5.14382363e-01 3.59592182e-02\n",
      "  7.94587921e-01 8.26272553e-01]\n",
      " [2.42542457e-02 6.21740693e-01 7.58180913e-01 7.50150533e-01\n",
      "  6.59438868e-02 5.43100543e-01 6.83854288e-01 7.95221091e-01\n",
      "  1.43931224e-01 8.37663272e-01]]\n",
      "Updated W2: [[0.7153943  0.82974447 0.64623125 0.61757326 0.63792886 0.3439728\n",
      "  0.12663809 0.32173387 0.19819603 0.82486394 0.90063628 0.6720469 ]\n",
      " [0.94129192 0.29720569 0.21375437 0.31539513 0.11892969 0.84327042\n",
      "  0.28338187 0.93964217 0.02148897 0.11947497 0.04438309 0.36989537]\n",
      " [0.98791343 0.82632795 0.87917344 0.10846422 0.85321304 0.41533832\n",
      "  0.49799902 0.79354033 0.86262421 0.92895122 0.82902687 0.40922892]\n",
      " [0.87572472 0.51875272 0.88345272 0.02039698 0.38325377 0.12078104\n",
      "  0.91512296 0.49380671 0.8689507  0.29294499 0.13169732 0.29300391]\n",
      " [0.09607137 0.20083715 0.22979096 0.67629571 0.92540049 0.98659867\n",
      "  0.27607512 0.99273788 0.92834563 0.72065234 0.62859445 0.21705202]\n",
      " [0.28654679 0.51861857 0.58997923 0.40789862 0.5879133  0.09837466\n",
      "  0.14274491 0.73145943 0.14973768 0.94060642 0.9972437  0.53698298]\n",
      " [0.02418269 0.26051696 0.77769607 0.36751608 0.46686389 0.56609435\n",
      "  0.78137999 0.2759871  0.8845656  0.10467828 0.82343354 0.65693716]\n",
      " [0.62538163 0.22476304 0.2722072  0.96698631 0.25565062 0.53421679\n",
      "  0.80212231 0.2186499  0.64328444 0.3608335  0.79273267 0.68651094]\n",
      " [0.16556145 0.07210811 0.62800187 0.74277969 0.8469567  0.46511706\n",
      "  0.70338469 0.33669488 0.60379229 0.15236194 0.90039638 0.03063558]\n",
      " [0.5616375  0.49441109 0.27616733 0.02827318 0.03956834 0.45978593\n",
      "  0.03832615 0.77974979 0.20259551 0.17566451 0.10369559 0.56139884]]\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Update Weights\n",
    "def backpropagate(W1, W2, hidden_layer, context_words, predictions, target_word, word_to_index, learning_rate=0.01):\n",
    "    target_vector = one_hot_encode(target_word, word_to_index)\n",
    "    error = predictions - target_vector\n",
    "    dW2 = np.outer(hidden_layer, error)\n",
    "    dW1 = np.outer(np.sum([one_hot_encode(word, word_to_index) for word in context_words], axis=0), np.dot(W2, error))\n",
    "    W1 -= learning_rate * dW1\n",
    "    W2 -= learning_rate * dW2\n",
    "    return W1, W2\n",
    "\n",
    "# Example weight update\n",
    "W1, W2 = backpropagate(W1, W2, hidden_layer, context_words, predictions, target_word, word_to_index)\n",
    "print(\"Updated W1:\", W1)\n",
    "print(\"Updated W2:\", W2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22ce2ad",
   "metadata": {},
   "source": [
    "## Training the CBOW Model\n",
    "\n",
    "We will train the CBOW model on the example text corpus for multiple epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43a9bbd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Loss: 8.1494\n",
      "Epoch 200, Loss: 8.1489\n",
      "Epoch 300, Loss: 8.1484\n",
      "Epoch 400, Loss: 8.1478\n",
      "Epoch 500, Loss: 8.1473\n",
      "Epoch 600, Loss: 8.1468\n",
      "Epoch 700, Loss: 8.1463\n",
      "Epoch 800, Loss: 8.1459\n",
      "Epoch 900, Loss: 8.1454\n",
      "Epoch 1000, Loss: 8.1449\n"
     ]
    }
   ],
   "source": [
    "# Training the CBOW Model\n",
    "for epoch in range(1000):\n",
    "    total_loss = 0\n",
    "    for context_words, target_word in training_data:\n",
    "        predictions, hidden_layer = forward_pass(context_words, W1, W2, word_to_index)\n",
    "        loss = calculate_loss(predictions, target_word, word_to_index)\n",
    "        total_loss += loss\n",
    "        W1, W2 = backpropagate(W1, W2, hidden_layer, context_words, predictions, target_word, word_to_index)\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch+100}, Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06a256dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000, Loss: 67.8641\n",
      "Epoch 2000, Loss: 8.5707\n",
      "Epoch 3000, Loss: 8.3330\n",
      "Epoch 4000, Loss: 8.2597\n",
      "Epoch 5000, Loss: 8.2227\n",
      "Epoch 6000, Loss: 8.1996\n",
      "Epoch 7000, Loss: 8.1836\n",
      "Epoch 8000, Loss: 8.1718\n",
      "Epoch 9000, Loss: 8.1626\n",
      "Epoch 10000, Loss: 8.1554\n"
     ]
    }
   ],
   "source": [
    "# Training the CBOW Model\n",
    "for epoch in range(10000):\n",
    "    total_loss = 0\n",
    "    for context_words, target_word in training_data:\n",
    "        predictions, hidden_layer = forward_pass(context_words, W1, W2, word_to_index)\n",
    "        loss = calculate_loss(predictions, target_word, word_to_index)\n",
    "        total_loss += loss\n",
    "        W1, W2 = backpropagate(W1, W2, hidden_layer, context_words, predictions, target_word, word_to_index)\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch+1000}, Loss: {total_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
